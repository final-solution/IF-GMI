---
target_model:
  architecture: densenet169 # architecture of target model
  num_classes: 1000 # number of output neurons
  weights: pretrained/densenet169_celeba.pt # link to weight file
evaluation_model:
  architecture: inception-v3 # architecture of evaluation model
  num_classes: 1000 # number of output neurons
  weights: pretrained/inception_v3_celeba.pt # link to weight file
augmented_models:
  architecture: [resnet152,resnext101]
  num_classes: 1000
  weights: [pretrained/resnet152_ffhq_distill.pt,pretrained/resnext101_ffhq_distill.pt]

stylegan_model: stylegan2_intermediate/ffhq_G.pth # Path to StyleGAN2 weight.
seed: 42 # Seed used for splitting the datasets and initialize the attack.
dataset: celeba_identities # Target dataset, select one of [facescrub, celeba_identities, stanford_dogs_cropped, stanford_dogs_uncropped].
result_path: results/intermediate

candidates:
  loss_fn: poincare # [ce, poincare, maxmargin, nll]
  num_candidates: 200 # Number of latent vectors to optimize for each target.
  candidate_search:
    search_space_size: 5000 # Set of randomly sampled latent vector, from which the candidates are selected.
    center_crop: 800 # Crop generated images.
    resize: 224 # Resize generated images (after cropping).
    horizontal_flip: true # Flip the generated images horizontally in 50% of the cases.
    batch_size: 25 # Batch size during the sampling process (single GPU).
    truncation_psi: 0.5 # Truncation psi for StyleGAN.
    truncation_cutoff: 8 # Truncation cutoff for StyleGAN.

attack:
  batch_size: 5 # Batch size per GPU.
  num_epochs: 70 # Number of optimization iterations per batch.
  targets: 10 # Specify the targeted classes, either a single class index, a list of indices, or all.
  discriminator_loss_weight: 0.0 # Add discriminator weight.
  single_w: true # Optimize a single 512-vector. Otherwise, a distinct vector for each AdaIn operation is optimized.
  clip: false # Clip generated images in range [-1, 1].
  augmentation_num: 0 # 使用增强模型的数量,0为不使用
  transformations: # Transformations applied during the optimization.
    CenterCrop:
      size: 800
    Resize:
      size: 224
      antialias: true
    RandomResizedCrop:
      size: [224, 224]
      scale: [0.9, 1.0]
      ratio: [1.0, 1.0]
      antialias: true

  optimizer: # Optimizer used for optimization. All optimizers from torch.optim are possible.
    Adam:
      lr: 0.005
      weight_decay: 0
      betas: [0.1, 0.1]

  # lr_scheduler: # Option to provide a learning rate scheduler from torch.optim.
  #   MultiStepLR:
  #     milestones: [30, 40]
  #     gamma: 0.1

# 中间层相关
intermediate:
  start: 0
  end: 8
  steps: [10,20,30,40,50,60,70,80]
  max_radius_mid_vecor: [1000, 2000, 3000, 4000, 5000, 6000, 8000, 8000]
  max_radius_w: [1000, 2000, 3000, 4000, 5000, 6000, 8000, 8000]

final_selection:
  samples_per_target: 50 # Number of samples to select from the set of optimized latent vectors.
  approach: transforms # Currently only transforms is available as an option.
  iterations: 100 # Number of iterations random transformations are applied.


wandb: # Options for WandB logging.
  enable_logging: true # Activate logging.
  wandb_init_args: # WandB init arguments.
    project: intermediate-MIA
    entity: model-inversion
    save_code: true
    name: intermediate-CelebA-10